{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb80bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def unir_csv_scrapeados(csv_files, output_dir=\"../data/merge_data\", output_filename=\"noticias_unidas.csv\"):\n",
    "    \"\"\"\n",
    "    Une múltiples archivos CSV de noticias scrapeadas en un solo archivo\n",
    "    \n",
    "    Args:\n",
    "        csv_files: lista de rutas de archivos CSV a unir\n",
    "        output_dir: directorio donde guardar el archivo unido\n",
    "        output_filename: nombre del archivo CSV resultante\n",
    "    \n",
    "    Returns:\n",
    "        str: ruta del archivo CSV resultante\n",
    "    \"\"\"\n",
    "    \n",
    "    # Crear directorio de salida si no existe\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    dataframes = []\n",
    "    \n",
    "    print(\"Cargando archivos CSV...\")\n",
    "    for csv_file in csv_files:\n",
    "        if os.path.exists(csv_file):\n",
    "            df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "            print(f\"✓ {os.path.basename(csv_file)}: {len(df)} registros\")\n",
    "            dataframes.append(df)\n",
    "        else:\n",
    "            print(f\"✗ Archivo no encontrado: {csv_file}\")\n",
    "    \n",
    "    if not dataframes:\n",
    "        print(\"No se encontraron archivos CSV válidos\")\n",
    "        return None\n",
    "    \n",
    "    # Unir todos los dataframes\n",
    "    df_unido = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Ruta del archivo de salida\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    # Guardar archivo unido\n",
    "    df_unido.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"RESUMEN DE LA UNIÓN\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total de registros: {len(df_unido)}\")\n",
    "    print(f\"Distribución por fuente:\")\n",
    "    print(df_unido['fuente'].value_counts())\n",
    "    print(f\"\\nDistribución por veracidad:\")\n",
    "    veracidad_counts = df_unido['veracidad'].value_counts()\n",
    "    print(f\"Verdaderas (1): {veracidad_counts.get(1, 0)}\")\n",
    "    print(f\"Falsas (0): {veracidad_counts.get(0, 0)}\")\n",
    "    print(f\"\\nArchivo guardado en: {output_path}\")\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fef53aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando archivos CSV...\n",
      "✓ noticiasColombiaCheck.csv: 75 registros\n",
      "✓ noticias_bbc.csv: 75 registros\n",
      "\n",
      "==================================================\n",
      "RESUMEN DE LA UNIÓN\n",
      "==================================================\n",
      "Total de registros: 150\n",
      "Distribución por fuente:\n",
      "fuente\n",
      "colombiaCheck    75\n",
      "bbc              75\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución por veracidad:\n",
      "Verdaderas (1): 75\n",
      "Falsas (0): 75\n",
      "\n",
      "Archivo guardado en: ../data/merge_data\\noticias_combinadas.csv\n"
     ]
    }
   ],
   "source": [
    "# Lista de archivos CSV a unir\n",
    "archivos_csv = [\n",
    "    \"../data_scrapeada/noticiasColombiaCheck.csv\",\n",
    "    \"../data_scrapeada/noticias_bbc.csv\"\n",
    "]\n",
    "\n",
    "# Unir los archivos CSV\n",
    "archivo_resultado = unir_csv_scrapeados(\n",
    "    csv_files=archivos_csv,\n",
    "    output_dir=\"../data/merge_data\",\n",
    "    output_filename=\"noticias_combinadas.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a47a8bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ANÁLISIS DETALLADO DEL DATASET UNIDO\n",
      "============================================================\n",
      "Dimensiones del dataset: (150, 5)\n",
      "Columnas: ['fuente', 'titulo', 'descripcion', 'url', 'veracidad']\n",
      "\n",
      "Información por fuente:\n",
      "  colombiaCheck: 75 total (0 verdaderas, 75 falsas)\n",
      "  bbc: 75 total (75 verdaderas, 0 falsas)\n",
      "\n",
      "Primeros 3 registros:\n",
      "----------------------------------------\n",
      "Fuente: colombiaCheck\n",
      "Título: Busto del expresidente Nieto en alocución de Petro fue reemplazado por el de Sta...\n",
      "Veracidad: Falsa\n",
      "----------------------------------------\n",
      "Fuente: colombiaCheck\n",
      "Título: Video de pesqueros chinos circula como si fuera la Flotilla Global Sumud rumbo a...\n",
      "Veracidad: Falsa\n",
      "----------------------------------------\n",
      "Fuente: colombiaCheck\n",
      "Título: Sanciones de la JEP a exjefes de las FARC por secuestro sí deben reparar y ser m...\n",
      "Veracidad: Falsa\n",
      "----------------------------------------\n",
      "\n",
      "Duplicados por título: 0\n",
      "Valores nulos por columna:\n",
      "fuente         0\n",
      "titulo         0\n",
      "descripcion    0\n",
      "url            0\n",
      "veracidad      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Análisis adicional del dataset unido\n",
    "if archivo_resultado and os.path.exists(archivo_resultado):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANÁLISIS DETALLADO DEL DATASET UNIDO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Cargar el dataset unido\n",
    "    df_final = pd.read_csv(archivo_resultado, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Dimensiones del dataset: {df_final.shape}\")\n",
    "    print(f\"Columnas: {list(df_final.columns)}\")\n",
    "    \n",
    "    print(f\"\\nInformación por fuente:\")\n",
    "    for fuente in df_final['fuente'].unique():\n",
    "        subset = df_final[df_final['fuente'] == fuente]\n",
    "        verdaderas = len(subset[subset['veracidad'] == 1])\n",
    "        falsas = len(subset[subset['veracidad'] == 0])\n",
    "        print(f\"  {fuente}: {len(subset)} total ({verdaderas} verdaderas, {falsas} falsas)\")\n",
    "    \n",
    "    print(f\"\\nPrimeros 3 registros:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, row in df_final.head(3).iterrows():\n",
    "        print(f\"Fuente: {row['fuente']}\")\n",
    "        print(f\"Título: {row['titulo'][:80]}{'...' if len(row['titulo']) > 80 else ''}\")\n",
    "        print(f\"Veracidad: {'Verdadera' if row['veracidad'] == 1 else 'Falsa'}\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    print(f\"\\nDuplicados por título: {df_final['titulo'].duplicated().sum()}\")\n",
    "    print(f\"Valores nulos por columna:\")\n",
    "    print(df_final.isnull().sum())\n",
    "else:\n",
    "    print(\"No se pudo crear o encontrar el archivo unido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc122b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERIFICACIÓN FINAL:\n",
      "Ruta esperada: ../data/merge_data/noticias_combinadas.csv\n",
      "Ruta completa: c:\\Users\\ABRAHAM\\Documents\\GitHub\\Practica-1\\data\\merge_data\\noticias_combinadas.csv\n",
      "Archivo existe: True\n",
      "Tamaño del archivo: 767,200 bytes (0.73 MB)\n",
      "Directorio de destino: c:\\Users\\ABRAHAM\\Documents\\GitHub\\Practica-1\\data\\merge_data\n",
      "Archivos en merge_data:\n",
      "  - full_dataset.csv (5,611,844 bytes)\n",
      "  - noticias_combinadas.csv (767,200 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Verificar que el archivo se guardó en la ubicación correcta\n",
    "expected_path = \"../data/merge_data/noticias_combinadas.csv\"\n",
    "full_path = os.path.abspath(expected_path)\n",
    "\n",
    "print(\"VERIFICACIÓN FINAL:\")\n",
    "print(f\"Ruta esperada: {expected_path}\")\n",
    "print(f\"Ruta completa: {full_path}\")\n",
    "print(f\"Archivo existe: {os.path.exists(expected_path)}\")\n",
    "\n",
    "if os.path.exists(expected_path):\n",
    "    file_size = os.path.getsize(expected_path)\n",
    "    print(f\"Tamaño del archivo: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)\")\n",
    "    \n",
    "    # Verificar que el directorio coincide con la estructura del proyecto\n",
    "    merge_dir = os.path.dirname(full_path)\n",
    "    print(f\"Directorio de destino: {merge_dir}\")\n",
    "    \n",
    "    # Mostrar archivos en el directorio merge_data\n",
    "    if os.path.exists(merge_dir):\n",
    "        print(f\"Archivos en {os.path.basename(merge_dir)}:\")\n",
    "        for file in os.listdir(merge_dir):\n",
    "            if file.endswith('.csv'):\n",
    "                file_path = os.path.join(merge_dir, file)\n",
    "                size = os.path.getsize(file_path)\n",
    "                print(f\"  - {file} ({size:,} bytes)\")\n",
    "else:\n",
    "    print(\"❌ El archivo no se creó correctamente\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
