{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abaa5490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio base del proyecto: C:\\Users\\ABRAHAM\\Documents\\GitHub\\Practica-1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/ABRAHAM/Documents/GitHub/Practica-1')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extracci√≥n de Caracter√≠sticas de Texto - An√°lisis de Noticias\n",
    "Autor: Abraham MD\n",
    "Fecha: 2025\n",
    "\"\"\"\n",
    "\n",
    "# === IMPORTACIONES ===\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuraci√≥n\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# === CONFIGURACI√ìN DE RUTAS ===\n",
    "BASE_DIR = pathlib.Path.cwd().parent.resolve()\n",
    "print(f\"Directorio base del proyecto: {BASE_DIR}\")\n",
    "\n",
    "# Verificar estructura de directorios\n",
    "\n",
    "\n",
    "\n",
    "BASE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826bf289",
   "metadata": {},
   "source": [
    "# Carga y An√°lisis del Dataset\n",
    "\n",
    "Este notebook implementa la extracci√≥n de caracter√≠sticas de texto para el an√°lisis de noticias verdaderas vs falsas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6cab4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrado dataset en: C:\\Users\\ABRAHAM\\Documents\\GitHub\\Practica-1\\data\\corpus_limpio\\noticias_combinadas.csv\n",
      "Dataset cargado: 5,518 registros\n",
      "Columnas: ['text', 'label']\n",
      "Distribuci√≥n de etiquetas:\n",
      "   ‚Ä¢ Verdaderas: 2,839 (51.4%)\n",
      "   ‚Ä¢ Falsas: 2,679 (48.6%)\n",
      "\n",
      "Vista previa del dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distintas desinformaciones senalan falsamente ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>el metraje realmente corresponde a embarcacion...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>una cuenta desinformadora de derecha adelanta ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>se trata de una suplantacion que no ha sido pu...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>un video viral atribuye a hugo ‚Äúel pollo‚Äù carv...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  distintas desinformaciones senalan falsamente ...  False\n",
       "1  el metraje realmente corresponde a embarcacion...  False\n",
       "2  una cuenta desinformadora de derecha adelanta ...  False\n",
       "3  se trata de una suplantacion que no ha sido pu...  False\n",
       "4  un video viral atribuye a hugo ‚Äúel pollo‚Äù carv...  False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === CARGA DEL DATASET LIMPIO ===\n",
    "def load_clean_dataset() -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Carga el dataset limpio desde m√∫ltiples ubicaciones posibles\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con los datos limpios o None si no se encuentra\n",
    "    \"\"\"\n",
    "    # Posibles ubicaciones del archivo\n",
    "    possible_paths = [\n",
    "        BASE_DIR / 'data' / 'corpus_limpio' / 'noticias_combinadas.csv'\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        try:\n",
    "            if path.exists():\n",
    "                print(f\"Encontrado dataset en: {path}\")\n",
    "                df = pd.read_csv(path, encoding='utf-8')\n",
    "                \n",
    "                \n",
    "                # Limpiar y preparar datos\n",
    "                df['text'] = df['text'].astype(str)\n",
    "                df['label'] = df['label'].astype(bool)\n",
    "                \n",
    "                # Eliminar textos vac√≠os\n",
    "                initial_len = len(df)\n",
    "                df = df[df['text'].str.strip() != ''].reset_index(drop=True)\n",
    "                removed = initial_len - len(df)\n",
    "                \n",
    "                if removed > 0:\n",
    "                    print(f\"Eliminados {removed} registros con texto vac√≠o\")\n",
    "                \n",
    "                print(f\"Dataset cargado: {len(df):,} registros\")\n",
    "                print(f\"Columnas: {list(df.columns)}\")\n",
    "                print(f\"Distribuci√≥n de etiquetas:\")\n",
    "                label_counts = df['label'].value_counts()\n",
    "                for label, count in label_counts.items():\n",
    "                    label_name = \"Verdaderas\" if label else \"Falsas\"\n",
    "                    pct = (count / len(df)) * 100\n",
    "                    print(f\"   ‚Ä¢ {label_name}: {count:,} ({pct:.1f}%)\")\n",
    "                \n",
    "                return df\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error cargando {path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"No se pudo cargar el dataset desde ninguna ubicaci√≥n\")\n",
    "    return None\n",
    "\n",
    "# Cargar datos\n",
    "df = load_clean_dataset()\n",
    "if df is not None:\n",
    "    print(f\"\\nVista previa del dataset:\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"No se puede continuar sin datos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f15c438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de SpaCy cargado exitosamente\n",
      "   ‚Ä¢ Idioma: es\n",
      "   ‚Ä¢ Pipeline: ['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "   ‚Ä¢ Stopwords disponibles: 521\n"
     ]
    }
   ],
   "source": [
    "# === INICIALIZACI√ìN DE SPACY ===\n",
    "def setup_spacy_model() -> Optional[spacy.Language]:\n",
    "    \"\"\"\n",
    "    Configura y carga el modelo de SpaCy para espa√±ol\n",
    "    \n",
    "    Returns:\n",
    "        Modelo de SpaCy o None si hay error\n",
    "    \"\"\"\n",
    "\n",
    "    # Intentar cargar el modelo\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "    \n",
    "    # Verificar que el modelo funciona\n",
    "    test_doc = nlp(\"Esta es una prueba del modelo de SpaCy.\")\n",
    "    if len(test_doc) > 0:\n",
    "        print(\"Modelo de SpaCy cargado exitosamente\")\n",
    "        print(f\"   ‚Ä¢ Idioma: {nlp.lang}\")\n",
    "        print(f\"   ‚Ä¢ Pipeline: {nlp.pipe_names}\")\n",
    "        print(f\"   ‚Ä¢ Stopwords disponibles: {len(nlp.Defaults.stop_words)}\")\n",
    "        return nlp\n",
    "    else:\n",
    "        print(\"El modelo no procesa texto correctamente\")\n",
    "        return None\n",
    "            \n",
    "\n",
    "\n",
    "# Configurar SpaCy\n",
    "nlp = setup_spacy_model()\n",
    "\n",
    "if nlp:\n",
    "    # Mostrar ejemplo de procesamiento\n",
    "    sample_text = \"Las noticias falsas son un problema grave en la sociedad moderna.\"\n",
    "    doc = nlp(sample_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d076cfd",
   "metadata": {},
   "source": [
    "# Tokenizaci√≥n y Stemming\n",
    "\n",
    "Procesamiento avanzado de texto utilizando SpaCy y NLTK para:\n",
    "- **Tokenizaci√≥n**: Separar el texto en unidades b√°sicas\n",
    "- **Stemming**: Reducir palabras a su ra√≠z lexical\n",
    "- **Filtrado**: Eliminar puntuaci√≥n y espacios irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5c60198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Procesando 5,518 textos...\n",
      "‚úÖ Procesamiento completado en 545.00 segundos\n",
      "üìä Estad√≠sticas:\n",
      "   ‚Ä¢ Textos vac√≠os despu√©s del procesamiento: 0\n",
      "   ‚Ä¢ Tokens promedio (antes): 182.2\n",
      "   ‚Ä¢ Tokens promedio (despu√©s): 172.8\n",
      "   ‚Ä¢ Reducci√≥n de tokens: 5.2%\n",
      "\n",
      "üîç Ejemplo de procesamiento:\n",
      "Original: distintas desinformaciones senalan falsamente la estatua como si fuera del dictador sovietico unas con la imagen original y otras con inteligencia artificial...\n",
      "Procesado: distint desinform senal fals la estatu com si fuer del dictador soviet unas con la imag original otras con inteligent artificial...\n",
      "‚úÖ Procesamiento completado en 545.00 segundos\n",
      "üìä Estad√≠sticas:\n",
      "   ‚Ä¢ Textos vac√≠os despu√©s del procesamiento: 0\n",
      "   ‚Ä¢ Tokens promedio (antes): 182.2\n",
      "   ‚Ä¢ Tokens promedio (despu√©s): 172.8\n",
      "   ‚Ä¢ Reducci√≥n de tokens: 5.2%\n",
      "\n",
      "üîç Ejemplo de procesamiento:\n",
      "Original: distintas desinformaciones senalan falsamente la estatua como si fuera del dictador sovietico unas con la imagen original y otras con inteligencia artificial...\n",
      "Procesado: distint desinform senal fals la estatu com si fuer del dictador soviet unas con la imag original otras con inteligent artificial...\n"
     ]
    }
   ],
   "source": [
    "# === PROCESAMIENTO DE TEXTO: TOKENIZACI√ìN Y STEMMING ===\n",
    "\n",
    "class TextProcessor:\n",
    "    \"\"\"\n",
    "    Clase para procesamiento avanzado de texto con SpaCy y NLTK\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nlp_model=None):\n",
    "        self.nlp = nlp_model\n",
    "        self.stemmer = SnowballStemmer('spanish')\n",
    "        \n",
    "    def tokenize_and_stem(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Tokeniza el texto y aplica stemming usando SpaCy + NLTK\n",
    "        \n",
    "        Args:\n",
    "            text (str): Texto a procesar\n",
    "            \n",
    "        Returns:\n",
    "            str: Texto tokenizado y con stemming aplicado\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        if self.nlp is None:\n",
    "            # Fallback sin SpaCy\n",
    "            words = text.split()\n",
    "            stemmed = [self.stemmer.stem(word) for word in words if word.isalpha()]\n",
    "            return \" \".join(stemmed)\n",
    "        \n",
    "        try:\n",
    "            # Procesar con SpaCy\n",
    "            doc = self.nlp(text)\n",
    "            \n",
    "            # Filtrar y aplicar stemming\n",
    "            processed_tokens = []\n",
    "            for token in doc:\n",
    "                # Filtrar tokens no deseados\n",
    "                if (not token.is_punct and \n",
    "                    not token.is_space and \n",
    "                    not token.is_digit and\n",
    "                    len(token.text.strip()) > 1 and\n",
    "                    token.text.isalpha()):\n",
    "                    \n",
    "                    # Aplicar stemming al token limpio\n",
    "                    stemmed_token = self.stemmer.stem(token.text.lower())\n",
    "                    processed_tokens.append(stemmed_token)\n",
    "            \n",
    "            return \" \".join(processed_tokens)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando texto: {e}\")\n",
    "            # Fallback simple\n",
    "            words = text.split()\n",
    "            return \" \".join([self.stemmer.stem(word.lower()) for word in words if word.isalpha()])\n",
    "    \n",
    "    def process_dataframe(self, df: pd.DataFrame, text_column: str = 'text') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Procesa una columna de texto en un DataFrame\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame a procesar\n",
    "            text_column: Nombre de la columna de texto\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame con columna adicional de texto procesado\n",
    "        \"\"\"\n",
    "        if df.empty or text_column not in df.columns:\n",
    "            print(f\"DataFrame vac√≠o o columna '{text_column}' no encontrada\")\n",
    "            return df\n",
    "        \n",
    "        print(f\"Procesando {len(df):,} textos...\")\n",
    "        \n",
    "        # Crear copia del DataFrame\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Aplicar procesamiento\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        df_processed['texto_procesado'] = df_processed[text_column].apply(self.tokenize_and_stem)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        # Estad√≠sticas\n",
    "        empty_processed = (df_processed['texto_procesado'] == \"\").sum()\n",
    "        avg_tokens_before = df_processed[text_column].str.split().str.len().mean()\n",
    "        avg_tokens_after = df_processed['texto_procesado'].str.split().str.len().mean()\n",
    "        \n",
    "        print(f\"Procesamiento completado en {processing_time:.2f} segundos\")\n",
    "        print(f\"Estad√≠sticas:\")\n",
    "        print(f\"   ‚Ä¢ Textos vac√≠os despu√©s del procesamiento: {empty_processed}\")\n",
    "        print(f\"   ‚Ä¢ Tokens promedio (antes): {avg_tokens_before:.1f}\")\n",
    "        print(f\"   ‚Ä¢ Tokens promedio (despu√©s): {avg_tokens_after:.1f}\")\n",
    "        print(f\"   ‚Ä¢ Reducci√≥n de tokens: {((avg_tokens_before - avg_tokens_after) / avg_tokens_before * 100):.1f}%\")\n",
    "        \n",
    "        return df_processed\n",
    "\n",
    "# Inicializar procesador\n",
    "if df is not None:\n",
    "    processor = TextProcessor(nlp)\n",
    "    \n",
    "    # Procesar el dataset\n",
    "    df_processed = processor.process_dataframe(df, 'text')\n",
    "\n",
    "    print(f\"\\nEjemplo de procesamiento:\")\n",
    "    if len(df_processed) > 0:\n",
    "        sample_idx = 0\n",
    "        original = df_processed.iloc[sample_idx]['text'][:200]\n",
    "        processed = df_processed.iloc[sample_idx]['texto_procesado'][:200]\n",
    "        print(f\"Original: {original}...\")\n",
    "        print(f\"Procesado: {processed}...\")\n",
    "else:\n",
    "    print(\"No hay datos para procesar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95a50232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset con texto procesado:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>texto_procesado</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distintas desinformaciones senalan falsamente ...</td>\n",
       "      <td>distint desinform senal fals la estatu com si ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>el metraje realmente corresponde a embarcacion...</td>\n",
       "      <td>el metraj realment correspond embarc zarp tras...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>una cuenta desinformadora de derecha adelanta ...</td>\n",
       "      <td>una cuent desinform de derech adelant dat fals...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>se trata de una suplantacion que no ha sido pu...</td>\n",
       "      <td>se trat de una suplant que no ha sid public po...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>un video viral atribuye a hugo ‚Äúel pollo‚Äù carv...</td>\n",
       "      <td>un vide viral atribu hug el poll carvajal fals...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  distintas desinformaciones senalan falsamente ...   \n",
       "1  el metraje realmente corresponde a embarcacion...   \n",
       "2  una cuenta desinformadora de derecha adelanta ...   \n",
       "3  se trata de una suplantacion que no ha sido pu...   \n",
       "4  un video viral atribuye a hugo ‚Äúel pollo‚Äù carv...   \n",
       "\n",
       "                                     texto_procesado  label  \n",
       "0  distint desinform senal fals la estatu com si ...  False  \n",
       "1  el metraj realment correspond embarc zarp tras...  False  \n",
       "2  una cuent desinform de derech adelant dat fals...  False  \n",
       "3  se trat de una suplant que no ha sid public po...  False  \n",
       "4  un vide viral atribu hug el poll carvajal fals...  False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estad√≠sticas de longitud (tokens procesados):\n",
      "   ‚Ä¢ Promedio: 172.8 tokens\n",
      "   ‚Ä¢ Mediana: 41.0 tokens\n",
      "   ‚Ä¢ M√≠nimo: 5 tokens\n",
      "   ‚Ä¢ M√°ximo: 4402 tokens\n"
     ]
    }
   ],
   "source": [
    "# === VISUALIZACI√ìN DEL DATASET PROCESADO ===\n",
    "if 'df_processed' in locals() and df_processed is not None:\n",
    "    print(\"Dataset con texto procesado:\")\n",
    "    display(df_processed[['text', 'texto_procesado', 'label']].head())\n",
    "    \n",
    "    # An√°lisis r√°pido de longitudes\n",
    "    if 'texto_procesado' in df_processed.columns:\n",
    "        longitudes = df_processed['texto_procesado'].str.split().str.len()\n",
    "        print(f\"\\nEstad√≠sticas de longitud (tokens procesados):\")\n",
    "        print(f\"   ‚Ä¢ Promedio: {longitudes.mean():.1f} tokens\")\n",
    "        print(f\"   ‚Ä¢ Mediana: {longitudes.median():.1f} tokens\")\n",
    "        print(f\"   ‚Ä¢ M√≠nimo: {longitudes.min()} tokens\")\n",
    "        print(f\"   ‚Ä¢ M√°ximo: {longitudes.max()} tokens\")\n",
    "else:\n",
    "    print(\"No hay datos procesados para mostrar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdada905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo caracter√≠sticas con Bag of Words...\n",
      "Extrayendo vocabulario b√°sico (unigramas + bigramas)...\n",
      "Extrayendo vocabulario b√°sico (unigramas + bigramas)...\n",
      "   ‚Ä¢ Vocabulario b√°sico: 1,000 t√©rminos\n",
      "Extrayendo vocabulario sin stopwords...\n",
      "   ‚Ä¢ Vocabulario b√°sico: 1,000 t√©rminos\n",
      "Extrayendo vocabulario sin stopwords...\n",
      "   ‚Ä¢ Vocabulario sin stopwords: 800 t√©rminos\n",
      "Extrayendo vocabulario filtrado por frecuencia...\n",
      "   ‚Ä¢ Vocabulario sin stopwords: 800 t√©rminos\n",
      "Extrayendo vocabulario filtrado por frecuencia...\n",
      "   ‚Ä¢ Vocabulario filtrado: 600 t√©rminos\n",
      "Extrayendo caracter√≠sticas TF-IDF...\n",
      "   ‚Ä¢ Vocabulario filtrado: 600 t√©rminos\n",
      "Extrayendo caracter√≠sticas TF-IDF...\n",
      "   ‚Ä¢ Vocabulario TF-IDF: 500 t√©rminos\n",
      "\n",
      "An√°lisis de superposici√≥n de vocabularios:\n",
      "--------------------------------------------------\n",
      "   ‚Ä¢ basico ‚à© sin_stopwords: 800 t√©rminos comunes\n",
      "   ‚Ä¢ basico ‚à© filtrado: 600 t√©rminos comunes\n",
      "   ‚Ä¢ basico ‚à© tfidf: 500 t√©rminos comunes\n",
      "   ‚Ä¢ sin_stopwords ‚à© basico: 800 t√©rminos comunes\n",
      "   ‚Ä¢ sin_stopwords ‚à© filtrado: 600 t√©rminos comunes\n",
      "   ‚Ä¢ sin_stopwords ‚à© tfidf: 500 t√©rminos comunes\n",
      "   ‚Ä¢ filtrado ‚à© basico: 600 t√©rminos comunes\n",
      "   ‚Ä¢ filtrado ‚à© sin_stopwords: 600 t√©rminos comunes\n",
      "   ‚Ä¢ filtrado ‚à© tfidf: 500 t√©rminos comunes\n",
      "   ‚Ä¢ tfidf ‚à© basico: 500 t√©rminos comunes\n",
      "   ‚Ä¢ tfidf ‚à© sin_stopwords: 500 t√©rminos comunes\n",
      "   ‚Ä¢ tfidf ‚à© filtrado: 500 t√©rminos comunes\n",
      "\n",
      "T√©rminos m√°s frecuentes por tipo:\n",
      "   ‚Ä¢ basico: distint, senal, fals, com, fuer...\n",
      "   ‚Ä¢ sin_stopwords: distint, senal, fals, com, fuer...\n",
      "   ‚Ä¢ filtrado: distint, senal, fals, com, fuer...\n",
      "   ‚Ä¢ tfidf: senal, com, fuer, imag, inici...\n",
      "\n",
      "Caracter√≠sticas extra√≠das para 5,518 documentos\n",
      "   ‚Ä¢ Vocabulario TF-IDF: 500 t√©rminos\n",
      "\n",
      "An√°lisis de superposici√≥n de vocabularios:\n",
      "--------------------------------------------------\n",
      "   ‚Ä¢ basico ‚à© sin_stopwords: 800 t√©rminos comunes\n",
      "   ‚Ä¢ basico ‚à© filtrado: 600 t√©rminos comunes\n",
      "   ‚Ä¢ basico ‚à© tfidf: 500 t√©rminos comunes\n",
      "   ‚Ä¢ sin_stopwords ‚à© basico: 800 t√©rminos comunes\n",
      "   ‚Ä¢ sin_stopwords ‚à© filtrado: 600 t√©rminos comunes\n",
      "   ‚Ä¢ sin_stopwords ‚à© tfidf: 500 t√©rminos comunes\n",
      "   ‚Ä¢ filtrado ‚à© basico: 600 t√©rminos comunes\n",
      "   ‚Ä¢ filtrado ‚à© sin_stopwords: 600 t√©rminos comunes\n",
      "   ‚Ä¢ filtrado ‚à© tfidf: 500 t√©rminos comunes\n",
      "   ‚Ä¢ tfidf ‚à© basico: 500 t√©rminos comunes\n",
      "   ‚Ä¢ tfidf ‚à© sin_stopwords: 500 t√©rminos comunes\n",
      "   ‚Ä¢ tfidf ‚à© filtrado: 500 t√©rminos comunes\n",
      "\n",
      "T√©rminos m√°s frecuentes por tipo:\n",
      "   ‚Ä¢ basico: distint, senal, fals, com, fuer...\n",
      "   ‚Ä¢ sin_stopwords: distint, senal, fals, com, fuer...\n",
      "   ‚Ä¢ filtrado: distint, senal, fals, com, fuer...\n",
      "   ‚Ä¢ tfidf: senal, com, fuer, imag, inici...\n",
      "\n",
      "Caracter√≠sticas extra√≠das para 5,518 documentos\n"
     ]
    }
   ],
   "source": [
    "# === EXTRACCI√ìN DE CARACTER√çSTICAS: BAG OF WORDS ===\n",
    "print(\"Extrayendo caracter√≠sticas con Bag of Words...\")\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Clase para extracci√≥n de caracter√≠sticas de texto\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nlp_model=None):\n",
    "        self.nlp = nlp_model\n",
    "        self.stop_words = list(nlp_model.Defaults.stop_words) if nlp_model else None\n",
    "        \n",
    "    def extract_bow_features(self, df: pd.DataFrame, text_column: str = 'texto_procesado') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extrae caracter√≠sticas usando Bag of Words con diferentes configuraciones\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame con texto procesado\n",
    "            text_column: Columna con texto a analizar\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame con caracter√≠sticas extra√≠das\n",
    "        \"\"\"\n",
    "        if df.empty or text_column not in df.columns:\n",
    "            print(f\"Error: DataFrame vac√≠o o columna '{text_column}' no encontrada\")\n",
    "            return df\n",
    "        \n",
    "        df_features = df.copy()\n",
    "        \n",
    "        # 1. BOW b√°sico (unigramas y bigramas)\n",
    "        print(\"Extrayendo vocabulario b√°sico (unigramas + bigramas)...\")\n",
    "        vectorizer_basic = CountVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=1000,  # Limitar caracter√≠sticas para eficiencia\n",
    "            min_df=2,  # Aparecer en al menos 2 documentos\n",
    "            stop_words=self.stop_words\n",
    "        )\n",
    "        \n",
    "        # Ajustar vectorizador a todos los textos\n",
    "        all_texts = df_features[text_column].fillna(\"\").tolist()\n",
    "        vectorizer_basic.fit(all_texts)\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Vocabulario b√°sico: {len(vectorizer_basic.vocabulary_):,} t√©rminos\")\n",
    "        \n",
    "        # 2. BOW sin stopwords personalizadas\n",
    "        print(\"Extrayendo vocabulario sin stopwords...\")\n",
    "        vectorizer_no_stop = CountVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=800,\n",
    "            min_df=3,\n",
    "            stop_words=self.stop_words\n",
    "        )\n",
    "        vectorizer_no_stop.fit(all_texts)\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Vocabulario sin stopwords: {len(vectorizer_no_stop.vocabulary_):,} t√©rminos\")\n",
    "        \n",
    "        # 3. BOW con filtrado por frecuencia\n",
    "        print(\"Extrayendo vocabulario filtrado por frecuencia...\")\n",
    "        vectorizer_freq = CountVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=600,\n",
    "            min_df=5,  # Debe aparecer en al menos 5 documentos\n",
    "            max_df=0.8,  # No m√°s del 80% de documentos\n",
    "            stop_words=self.stop_words\n",
    "        )\n",
    "        vectorizer_freq.fit(all_texts)\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Vocabulario filtrado: {len(vectorizer_freq.vocabulary_):,} t√©rminos\")\n",
    "        \n",
    "        # 4. TF-IDF para comparaci√≥n\n",
    "        print(\"Extrayendo caracter√≠sticas TF-IDF...\")\n",
    "        vectorizer_tfidf = TfidfVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=500,\n",
    "            min_df=3,\n",
    "            max_df=0.8,\n",
    "            stop_words=self.stop_words\n",
    "        )\n",
    "        vectorizer_tfidf.fit(all_texts)\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Vocabulario TF-IDF: {len(vectorizer_tfidf.vocabulary_):,} t√©rminos\")\n",
    "        \n",
    "        # Guardar metadatos de vocabularios para an√°lisis posterior\n",
    "        df_features['vocab_basico'] = [len(vectorizer_basic.vocabulary_)] * len(df_features)\n",
    "        df_features['vocab_sin_stopwords'] = [len(vectorizer_no_stop.vocabulary_)] * len(df_features)\n",
    "        df_features['vocab_filtrado'] = [len(vectorizer_freq.vocabulary_)] * len(df_features)\n",
    "        df_features['vocab_tfidf'] = [len(vectorizer_tfidf.vocabulary_)] * len(df_features)\n",
    "        \n",
    "        # Almacenar vectorizadores para uso posterior\n",
    "        self.vectorizers = {\n",
    "            'basico': vectorizer_basic,\n",
    "            'sin_stopwords': vectorizer_no_stop,\n",
    "            'filtrado': vectorizer_freq,\n",
    "            'tfidf': vectorizer_tfidf\n",
    "        }\n",
    "        \n",
    "        return df_features\n",
    "    \n",
    "    def analyze_vocabulary_overlap(self) -> None:\n",
    "        \"\"\"\n",
    "        Analiza la superposici√≥n entre diferentes vocabularios\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'vectorizers'):\n",
    "            print(\"Primero ejecuta extract_bow_features()\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nAn√°lisis de superposici√≥n de vocabularios:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        vocabs = {name: set(vec.vocabulary_.keys()) for name, vec in self.vectorizers.items()}\n",
    "        \n",
    "        # Calcular intersecciones\n",
    "        intersections = {}\n",
    "        for name1, vocab1 in vocabs.items():\n",
    "            for name2, vocab2 in vocabs.items():\n",
    "                if name1 != name2:\n",
    "                    key = f\"{name1} ‚à© {name2}\"\n",
    "                    intersections[key] = len(vocab1 & vocab2)\n",
    "        \n",
    "        # Mostrar resultados\n",
    "        for key, value in intersections.items():\n",
    "            print(f\"   ‚Ä¢ {key}: {value:,} t√©rminos comunes\")\n",
    "        \n",
    "        # T√©rminos √∫nicos de cada vocabulario\n",
    "        print(f\"\\nT√©rminos m√°s frecuentes por tipo:\")\n",
    "        for name, vectorizer in self.vectorizers.items():\n",
    "            if hasattr(vectorizer, 'vocabulary_'):\n",
    "                # Obtener algunos t√©rminos de ejemplo\n",
    "                sample_terms = list(vectorizer.vocabulary_.keys())[:10]\n",
    "                print(f\"   ‚Ä¢ {name}: {', '.join(sample_terms[:5])}...\")\n",
    "\n",
    "# Ejecutar extracci√≥n de caracter√≠sticas\n",
    "if 'df_processed' in locals() and df_processed is not None:\n",
    "    extractor = FeatureExtractor(nlp)\n",
    "    df_with_features = extractor.extract_bow_features(df_processed, 'texto_procesado')\n",
    "    \n",
    "    # An√°lizar vocabularios\n",
    "    extractor.analyze_vocabulary_overlap()\n",
    "    \n",
    "    print(f\"\\nCaracter√≠sticas extra√≠das para {len(df_with_features):,} documentos\")\n",
    "else:\n",
    "    print(\"No hay datos procesados para extraer caracter√≠sticas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b87637f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset con caracter√≠sticas extra√≠das:\n",
      "Dimensiones: (5518, 7)\n",
      "Columnas: ['text', 'label', 'texto_procesado', 'vocab_basico', 'vocab_sin_stopwords', 'vocab_filtrado', 'vocab_tfidf']\n",
      "\n",
      "Vista previa (columnas: text, texto_procesado, label, vocab_basico, vocab_sin_stopwords):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>texto_procesado</th>\n",
       "      <th>label</th>\n",
       "      <th>vocab_basico</th>\n",
       "      <th>vocab_sin_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distintas desinformaciones senalan falsamente ...</td>\n",
       "      <td>distint desinform senal fals la estatu com si ...</td>\n",
       "      <td>False</td>\n",
       "      <td>1000</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>el metraje realmente corresponde a embarcacion...</td>\n",
       "      <td>el metraj realment correspond embarc zarp tras...</td>\n",
       "      <td>False</td>\n",
       "      <td>1000</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>una cuenta desinformadora de derecha adelanta ...</td>\n",
       "      <td>una cuent desinform de derech adelant dat fals...</td>\n",
       "      <td>False</td>\n",
       "      <td>1000</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>se trata de una suplantacion que no ha sido pu...</td>\n",
       "      <td>se trat de una suplant que no ha sid public po...</td>\n",
       "      <td>False</td>\n",
       "      <td>1000</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>un video viral atribuye a hugo ‚Äúel pollo‚Äù carv...</td>\n",
       "      <td>un vide viral atribu hug el poll carvajal fals...</td>\n",
       "      <td>False</td>\n",
       "      <td>1000</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  distintas desinformaciones senalan falsamente ...   \n",
       "1  el metraje realmente corresponde a embarcacion...   \n",
       "2  una cuenta desinformadora de derecha adelanta ...   \n",
       "3  se trata de una suplantacion que no ha sido pu...   \n",
       "4  un video viral atribuye a hugo ‚Äúel pollo‚Äù carv...   \n",
       "\n",
       "                                     texto_procesado  label  vocab_basico  \\\n",
       "0  distint desinform senal fals la estatu com si ...  False          1000   \n",
       "1  el metraj realment correspond embarc zarp tras...  False          1000   \n",
       "2  una cuent desinform de derech adelant dat fals...  False          1000   \n",
       "3  se trat de una suplant que no ha sid public po...  False          1000   \n",
       "4  un vide viral atribu hug el poll carvajal fals...  False          1000   \n",
       "\n",
       "   vocab_sin_stopwords  \n",
       "0                  800  \n",
       "1                  800  \n",
       "2                  800  \n",
       "3                  800  \n",
       "4                  800  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen estad√≠stico de caracter√≠sticas num√©ricas:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab_basico</th>\n",
       "      <th>vocab_sin_stopwords</th>\n",
       "      <th>vocab_filtrado</th>\n",
       "      <th>vocab_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5518.0</td>\n",
       "      <td>5518.0</td>\n",
       "      <td>5518.0</td>\n",
       "      <td>5518.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       vocab_basico  vocab_sin_stopwords  vocab_filtrado  vocab_tfidf\n",
       "count        5518.0               5518.0          5518.0       5518.0\n",
       "mean         1000.0                800.0           600.0        500.0\n",
       "std             0.0                  0.0             0.0          0.0\n",
       "min          1000.0                800.0           600.0        500.0\n",
       "25%          1000.0                800.0           600.0        500.0\n",
       "50%          1000.0                800.0           600.0        500.0\n",
       "75%          1000.0                800.0           600.0        500.0\n",
       "max          1000.0                800.0           600.0        500.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === VISUALIZACI√ìN DEL DATASET CON CARACTER√çSTICAS ===\n",
    "if 'df_with_features' in locals() and df_with_features is not None:\n",
    "    print(\"Dataset con caracter√≠sticas extra√≠das:\")\n",
    "    \n",
    "    # Mostrar informaci√≥n del dataset\n",
    "    print(f\"Dimensiones: {df_with_features.shape}\")\n",
    "    print(f\"Columnas: {list(df_with_features.columns)}\")\n",
    "\n",
    "    # Mostrar vista previa\n",
    "    display_columns = ['text', 'texto_procesado', 'label', 'vocab_basico', 'vocab_sin_stopwords']\n",
    "    available_columns = [col for col in display_columns if col in df_with_features.columns]\n",
    "    \n",
    "    if available_columns:\n",
    "        print(f\"\\nVista previa (columnas: {', '.join(available_columns)}):\")\n",
    "        display(df_with_features[available_columns].head())\n",
    "    else:\n",
    "        display(df_with_features.head())\n",
    "    \n",
    "    # Resumen estad√≠stico de caracter√≠sticas\n",
    "    numeric_cols = df_with_features.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\nResumen estad√≠stico de caracter√≠sticas num√©ricas:\")\n",
    "        display(df_with_features[numeric_cols].describe())\n",
    "else:\n",
    "    print(\"No hay dataset con caracter√≠sticas para mostrar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "328f9b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo pr√°ctico de vectorizaci√≥n...\n",
      "Textos de ejemplo:\n",
      "   1. El zorro marr√≥n salta sobre el perro perezoso. El zorro es muy r√°pido y astuto.\n",
      "   2. Las noticias falsas se propagan r√°pidamente en redes sociales modernas.\n",
      "   3. La inteligencia artificial ayuda a detectar informaci√≥n falsa autom√°ticamente.\n",
      "\n",
      "Vocabulario extra√≠do (40 t√©rminos):\n",
      "   Primeros 10 t√©rminos:\n",
      "     'artificial' -> √≠ndice 0\n",
      "     'artificial ayuda' -> √≠ndice 1\n",
      "     'astuto' -> √≠ndice 2\n",
      "     'autom√°ticamente' -> √≠ndice 3\n",
      "     'ayuda' -> √≠ndice 4\n",
      "     'ayuda detectar' -> √≠ndice 5\n",
      "     'detectar' -> √≠ndice 6\n",
      "     'detectar informaci√≥n' -> √≠ndice 7\n",
      "     'falsa' -> √≠ndice 8\n",
      "     'falsa autom√°ticamente' -> √≠ndice 9\n",
      "\n",
      "Matriz de caracter√≠sticas (shape: (3, 40)):\n",
      "   Caracter√≠sticas mostradas: artificial, artificial ayuda, astuto, autom√°ticamente, ayuda, ayuda detectar, detectar, detectar informaci√≥n, falsa, falsa autom√°ticamente\n",
      "   Matriz (primeras 10 columnas):\n",
      "     Texto 1: [0 0 1 0 0 0 0 0 0 0]\n",
      "     Texto 2: [0 0 0 0 0 0 0 0 0 0]\n",
      "     Texto 3: [1 1 0 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# === EJEMPLO DE VECTORIZACI√ìN CON TEXTO DE PRUEBA ===\n",
    "print(\"Ejemplo pr√°ctico de vectorizaci√≥n...\")\n",
    "\n",
    "def demonstrate_vectorization():\n",
    "    \"\"\"\n",
    "    Demuestra c√≥mo funciona la vectorizaci√≥n con un ejemplo concreto\n",
    "    \"\"\"\n",
    "    # Texto de ejemplo\n",
    "    sample_texts = [\n",
    "        \"El zorro marr√≥n salta sobre el perro perezoso. El zorro es muy r√°pido y astuto.\",\n",
    "        \"Las noticias falsas se propagan r√°pidamente en redes sociales modernas.\",\n",
    "        \"La inteligencia artificial ayuda a detectar informaci√≥n falsa autom√°ticamente.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Textos de ejemplo:\")\n",
    "    for i, text in enumerate(sample_texts, 1):\n",
    "        print(f\"   {i}. {text}\")\n",
    "    \n",
    "    if nlp is not None:\n",
    "        # Usar el vectorizador configurado anteriormente\n",
    "        if 'extractor' in locals() and hasattr(extractor, 'vectorizers'):\n",
    "            vectorizer = extractor.vectorizers['basico']\n",
    "        else:\n",
    "            # Crear vectorizador simple para demostraci√≥n\n",
    "            vectorizer = CountVectorizer(\n",
    "                ngram_range=(1, 2),\n",
    "                stop_words=list(nlp.Defaults.stop_words) if nlp else None\n",
    "            )\n",
    "            vectorizer.fit(sample_texts)\n",
    "        \n",
    "        print(f\"\\nVocabulario extra√≠do ({len(vectorizer.vocabulary_)} t√©rminos):\")\n",
    "        \n",
    "        # Mostrar algunos t√©rminos del vocabulario\n",
    "        vocab_items = list(vectorizer.vocabulary_.items())\n",
    "        vocab_items.sort(key=lambda x: x[1])  # Ordenar por √≠ndice\n",
    "        \n",
    "        print(\"   Primeros 10 t√©rminos:\")\n",
    "        for term, idx in vocab_items[:10]:\n",
    "            print(f\"     '{term}' -> √≠ndice {idx}\")\n",
    "        \n",
    "        # Vectorizar los textos de ejemplo\n",
    "        print(f\"\\nMatriz de caracter√≠sticas (shape: {vectorizer.transform(sample_texts).shape}):\")\n",
    "        feature_matrix = vectorizer.transform(sample_texts).toarray()\n",
    "        \n",
    "        # Mostrar matriz con nombres de caracter√≠sticas\n",
    "        feature_names = vectorizer.get_feature_names_out()[:10]  # Primeras 10\n",
    "        print(f\"   Caracter√≠sticas mostradas: {', '.join(feature_names)}\")\n",
    "        print(\"   Matriz (primeras 10 columnas):\")\n",
    "        for i, row in enumerate(feature_matrix[:, :10]):\n",
    "            print(f\"     Texto {i+1}: {row}\")\n",
    "    else:\n",
    "        print(\"SpaCy no disponible, ejemplo limitado\")\n",
    "        \n",
    "        # Ejemplo b√°sico sin SpaCy\n",
    "        basic_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "        basic_vectorizer.fit(sample_texts)\n",
    "\n",
    "        print(f\"Vocabulario b√°sico ({len(basic_vectorizer.vocabulary_)} t√©rminos):\")\n",
    "        vocab_sample = list(basic_vectorizer.vocabulary_.keys())[:10]\n",
    "        print(f\"   Muestra: {', '.join(vocab_sample)}\")\n",
    "\n",
    "# Ejecutar demostraci√≥n\n",
    "demonstrate_vectorization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d004020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESUMEN FINAL DEL PROCESAMIENTO\n",
      "============================================================\n",
      "Registros totales: 5,518\n",
      "Columnas generadas: 7\n",
      "\n",
      "An√°lisis del texto procesado:\n",
      "An√°lisis del texto procesado:\n",
      "   ‚Ä¢ Longitud promedio: 172.8 tokens\n",
      "   ‚Ä¢ Longitud mediana: 41.0 tokens\n",
      "   ‚Ä¢ Rango: 5 - 4402 tokens\n",
      "\n",
      "Distribuci√≥n por tipo de noticia:\n",
      "\n",
      "   ‚Ä¢ Longitud promedio: 172.8 tokens\n",
      "   ‚Ä¢ Longitud mediana: 41.0 tokens\n",
      "   ‚Ä¢ Rango: 5 - 4402 tokens\n",
      "\n",
      "Distribuci√≥n por tipo de noticia:\n",
      "   ‚Ä¢ Falsas: 2,679 textos, 131.4 tokens promedio\n",
      "   ‚Ä¢ Verdaderas: 2,839 textos, 211.8 tokens promedio\n",
      "\n",
      "Tama√±os de vocabularios extra√≠dos:\n",
      "   ‚Ä¢ Basico: 1,000 t√©rminos\n",
      "   ‚Ä¢ Sin Stopwords: 800 t√©rminos\n",
      "   ‚Ä¢ Filtrado: 600 t√©rminos\n",
      "   ‚Ä¢ Tfidf: 500 t√©rminos\n",
      "\n",
      "Estructura final del dataset:\n",
      "Columnas: ['text', 'label', 'texto_procesado', 'vocab_basico', 'vocab_sin_stopwords', 'vocab_filtrado', 'vocab_tfidf']\n",
      "   ‚Ä¢ Falsas: 2,679 textos, 131.4 tokens promedio\n",
      "   ‚Ä¢ Verdaderas: 2,839 textos, 211.8 tokens promedio\n",
      "\n",
      "Tama√±os de vocabularios extra√≠dos:\n",
      "   ‚Ä¢ Basico: 1,000 t√©rminos\n",
      "   ‚Ä¢ Sin Stopwords: 800 t√©rminos\n",
      "   ‚Ä¢ Filtrado: 600 t√©rminos\n",
      "   ‚Ä¢ Tfidf: 500 t√©rminos\n",
      "\n",
      "Estructura final del dataset:\n",
      "Columnas: ['text', 'label', 'texto_procesado', 'vocab_basico', 'vocab_sin_stopwords', 'vocab_filtrado', 'vocab_tfidf']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>texto_procesado</th>\n",
       "      <th>vocab_basico</th>\n",
       "      <th>vocab_sin_stopwords</th>\n",
       "      <th>vocab_filtrado</th>\n",
       "      <th>vocab_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distintas desinformaciones senalan falsamente ...</td>\n",
       "      <td>False</td>\n",
       "      <td>distint desinform senal fals la estatu com si ...</td>\n",
       "      <td>1000</td>\n",
       "      <td>800</td>\n",
       "      <td>600</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>el metraje realmente corresponde a embarcacion...</td>\n",
       "      <td>False</td>\n",
       "      <td>el metraj realment correspond embarc zarp tras...</td>\n",
       "      <td>1000</td>\n",
       "      <td>800</td>\n",
       "      <td>600</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>una cuenta desinformadora de derecha adelanta ...</td>\n",
       "      <td>False</td>\n",
       "      <td>una cuent desinform de derech adelant dat fals...</td>\n",
       "      <td>1000</td>\n",
       "      <td>800</td>\n",
       "      <td>600</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  distintas desinformaciones senalan falsamente ...  False   \n",
       "1  el metraje realmente corresponde a embarcacion...  False   \n",
       "2  una cuenta desinformadora de derecha adelanta ...  False   \n",
       "\n",
       "                                     texto_procesado  vocab_basico  \\\n",
       "0  distint desinform senal fals la estatu com si ...          1000   \n",
       "1  el metraj realment correspond embarc zarp tras...          1000   \n",
       "2  una cuent desinform de derech adelant dat fals...          1000   \n",
       "\n",
       "   vocab_sin_stopwords  vocab_filtrado  vocab_tfidf  \n",
       "0                  800             600          500  \n",
       "1                  800             600          500  \n",
       "2                  800             600          500  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === AN√ÅLISIS FINAL DEL DATASET PROCESADO ===\n",
    "if 'df_with_features' in locals() and df_with_features is not None:\n",
    "    print(\"RESUMEN FINAL DEL PROCESAMIENTO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Informaci√≥n general\n",
    "    print(f\"Registros totales: {len(df_with_features):,}\")\n",
    "    print(f\"Columnas generadas: {len(df_with_features.columns)}\")\n",
    "    \n",
    "    # An√°lisis de texto procesado\n",
    "    if 'texto_procesado' in df_with_features.columns:\n",
    "        texto_procesado = df_with_features['texto_procesado']\n",
    "        \n",
    "        # Estad√≠sticas de longitud\n",
    "        longitudes = texto_procesado.str.split().str.len()\n",
    "        print(f\"\\nAn√°lisis del texto procesado:\")\n",
    "        print(f\"   ‚Ä¢ Longitud promedio: {longitudes.mean():.1f} tokens\")\n",
    "        print(f\"   ‚Ä¢ Longitud mediana: {longitudes.median():.1f} tokens\")\n",
    "        print(f\"   ‚Ä¢ Rango: {longitudes.min()} - {longitudes.max()} tokens\")\n",
    "        \n",
    "        # Textos vac√≠os\n",
    "        textos_vacios = (texto_procesado.str.strip() == '').sum()\n",
    "        if textos_vacios > 0:\n",
    "            print(f\"   Textos vac√≠os despu√©s del procesamiento: {textos_vacios}\")\n",
    "        \n",
    "        # Distribuci√≥n por etiquetas\n",
    "        if 'label' in df_with_features.columns:\n",
    "            print(f\"\\nDistribuci√≥n por tipo de noticia:\")\n",
    "            for label in [False, True]:\n",
    "                subset = df_with_features[df_with_features['label'] == label]\n",
    "                if len(subset) > 0:\n",
    "                    avg_length = subset['texto_procesado'].str.split().str.len().mean()\n",
    "                    label_name = \"Falsas\" if not label else \"Verdaderas\"\n",
    "                    print(f\"   ‚Ä¢ {label_name}: {len(subset):,} textos, {avg_length:.1f} tokens promedio\")\n",
    "    \n",
    "    # Informaci√≥n de vocabularios\n",
    "    vocab_cols = [col for col in df_with_features.columns if col.startswith('vocab_')]\n",
    "    if vocab_cols:\n",
    "        print(f\"\\nTama√±os de vocabularios extra√≠dos:\")\n",
    "        for col in vocab_cols:\n",
    "            vocab_size = df_with_features[col].iloc[0] if len(df_with_features) > 0 else 0\n",
    "            vocab_name = col.replace('vocab_', '').replace('_', ' ').title()\n",
    "            print(f\"   ‚Ä¢ {vocab_name}: {vocab_size:,} t√©rminos\")\n",
    "    \n",
    "    # Vista final del dataset\n",
    "    print(f\"\\nEstructura final del dataset:\")\n",
    "    print(f\"Columnas: {list(df_with_features.columns)}\")\n",
    "    \n",
    "    display(df_with_features.head(3))\n",
    "    \n",
    "else:\n",
    "    print(\"No hay dataset procesado para mostrar el resumen final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "330f8a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exportando dataset con caracter√≠sticas extra√≠das...\n",
      "Dataset exportado exitosamente:\n",
      "   Ubicaci√≥n: C:\\Users\\ABRAHAM\\Documents\\GitHub\\Practica-1\\data\\extraccion_caracteristicas\\extraccion_caracteristicas.csv\n",
      "   Registros: 5,518\n",
      "   Columnas: 7\n",
      "   Tama√±o: 10.51 MB\n",
      "Verificaci√≥n de integridad: EXITOSA\n",
      "\n",
      "Columnas exportadas:\n",
      "    1. text                      (object) - 5,518 valores no nulos\n",
      "    2. label                     (bool) - 5,518 valores no nulos\n",
      "    3. texto_procesado           (object) - 5,518 valores no nulos\n",
      "    4. vocab_basico              (int64) - 5,518 valores no nulos\n",
      "    5. vocab_sin_stopwords       (int64) - 5,518 valores no nulos\n",
      "    6. vocab_filtrado            (int64) - 5,518 valores no nulos\n",
      "    7. vocab_tfidf               (int64) - 5,518 valores no nulos\n",
      "\n",
      "============================================================\n",
      "REPORTE DE PROCESAMIENTO DE CARACTER√çSTICAS\n",
      "============================================================\n",
      "Reducci√≥n de palabras: 5.2%\n",
      "   ‚Ä¢ Palabras originales: 1,005,398\n",
      "   ‚Ä¢ Palabras procesadas: 953,324\n",
      "Caracter√≠sticas extra√≠das: 4 tipos de vocabulario\n",
      "Calidad de procesamiento:\n",
      "   ‚Ä¢ Textos vac√≠os iniciales: 0\n",
      "   ‚Ä¢ Textos vac√≠os finales: 0\n",
      "Procesamiento de caracter√≠sticas completado exitosamente\n",
      "Dataset exportado exitosamente:\n",
      "   Ubicaci√≥n: C:\\Users\\ABRAHAM\\Documents\\GitHub\\Practica-1\\data\\extraccion_caracteristicas\\extraccion_caracteristicas.csv\n",
      "   Registros: 5,518\n",
      "   Columnas: 7\n",
      "   Tama√±o: 10.51 MB\n",
      "Verificaci√≥n de integridad: EXITOSA\n",
      "\n",
      "Columnas exportadas:\n",
      "    1. text                      (object) - 5,518 valores no nulos\n",
      "    2. label                     (bool) - 5,518 valores no nulos\n",
      "    3. texto_procesado           (object) - 5,518 valores no nulos\n",
      "    4. vocab_basico              (int64) - 5,518 valores no nulos\n",
      "    5. vocab_sin_stopwords       (int64) - 5,518 valores no nulos\n",
      "    6. vocab_filtrado            (int64) - 5,518 valores no nulos\n",
      "    7. vocab_tfidf               (int64) - 5,518 valores no nulos\n",
      "\n",
      "============================================================\n",
      "REPORTE DE PROCESAMIENTO DE CARACTER√çSTICAS\n",
      "============================================================\n",
      "Reducci√≥n de palabras: 5.2%\n",
      "   ‚Ä¢ Palabras originales: 1,005,398\n",
      "   ‚Ä¢ Palabras procesadas: 953,324\n",
      "Caracter√≠sticas extra√≠das: 4 tipos de vocabulario\n",
      "Calidad de procesamiento:\n",
      "   ‚Ä¢ Textos vac√≠os iniciales: 0\n",
      "   ‚Ä¢ Textos vac√≠os finales: 0\n",
      "Procesamiento de caracter√≠sticas completado exitosamente\n"
     ]
    }
   ],
   "source": [
    "# === EXPORTACI√ìN DEL DATASET CON CARACTER√çSTICAS ===\n",
    "print(\"Exportando dataset con caracter√≠sticas extra√≠das...\")\n",
    "\n",
    "def export_processed_dataset(df: pd.DataFrame, base_dir: pathlib.Path) -> bool:\n",
    "    \"\"\"\n",
    "    Exporta el dataset procesado con manejo robusto de directorios y errores\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame a exportar\n",
    "        base_dir: Directorio base del proyecto\n",
    "        \n",
    "    Returns:\n",
    "        bool: True si la exportaci√≥n fue exitosa\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"Error: DataFrame vac√≠o, no se puede exportar\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Crear directorio de destino\n",
    "        output_dir = base_dir / 'data' / 'extraccion_caracteristicas'\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Ruta completa del archivo\n",
    "        output_path = output_dir / 'extraccion_caracteristicas.csv'\n",
    "        \n",
    "        # Exportar con manejo de encoding\n",
    "        df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        \n",
    "        # Verificar exportaci√≥n\n",
    "        file_size = output_path.stat().st_size\n",
    "        file_size_mb = file_size / (1024 * 1024)\n",
    "        \n",
    "        print(f\"Dataset exportado exitosamente:\")\n",
    "        print(f\"   Ubicaci√≥n: {output_path}\")\n",
    "        print(f\"   Registros: {len(df):,}\")\n",
    "        print(f\"   Columnas: {len(df.columns)}\")\n",
    "        print(f\"   Tama√±o: {file_size_mb:.2f} MB\")\n",
    "        \n",
    "        # Verificar integridad del archivo\n",
    "        try:\n",
    "            verification_df = pd.read_csv(output_path, encoding='utf-8', nrows=3)\n",
    "            if len(verification_df) > 0:\n",
    "                print(\"Verificaci√≥n de integridad: EXITOSA\")\n",
    "            else:\n",
    "                print(\"Archivo exportado est√° vac√≠o\")\n",
    "                return False\n",
    "        except Exception as verify_error:\n",
    "            print(f\"Error en verificaci√≥n: {verify_error}\")\n",
    "            return False\n",
    "        \n",
    "        # Resumen de columnas exportadas\n",
    "        print(f\"\\nColumnas exportadas:\")\n",
    "        for i, col in enumerate(df.columns, 1):\n",
    "            col_type = df[col].dtype\n",
    "            non_null = df[col].notna().sum()\n",
    "            print(f\"   {i:2}. {col:<25} ({col_type}) - {non_null:,} valores no nulos\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except PermissionError:\n",
    "        print(\"Error: Sin permisos para escribir en el directorio\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error inesperado durante la exportaci√≥n: {e}\")\n",
    "        return False\n",
    "\n",
    "def generate_processing_report(original_df: pd.DataFrame, processed_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Genera un reporte del procesamiento realizado\n",
    "    \"\"\"\n",
    "    if original_df.empty or processed_df.empty:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"REPORTE DE PROCESAMIENTO DE CARACTER√çSTICAS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Estad√≠sticas de transformaci√≥n\n",
    "    original_words = original_df['text'].astype(str).str.split().str.len().sum()\n",
    "    if 'texto_procesado' in processed_df.columns:\n",
    "        processed_words = processed_df['texto_procesado'].str.split().str.len().sum()\n",
    "        word_reduction = ((original_words - processed_words) / original_words) * 100\n",
    "        print(f\"Reducci√≥n de palabras: {word_reduction:.1f}%\")\n",
    "        print(f\"   ‚Ä¢ Palabras originales: {original_words:,}\")\n",
    "        print(f\"   ‚Ä¢ Palabras procesadas: {processed_words:,}\")\n",
    "    \n",
    "    # Caracter√≠sticas extra√≠das\n",
    "    feature_cols = [col for col in processed_df.columns if col.startswith('vocab_')]\n",
    "    if feature_cols:\n",
    "        print(f\"Caracter√≠sticas extra√≠das: {len(feature_cols)} tipos de vocabulario\")\n",
    "    \n",
    "    # Calidad de datos\n",
    "    original_empty = (original_df['text'].astype(str).str.strip() == '').sum()\n",
    "    if 'texto_procesado' in processed_df.columns:\n",
    "        processed_empty = (processed_df['texto_procesado'].str.strip() == '').sum()\n",
    "        print(f\"Calidad de procesamiento:\")\n",
    "        print(f\"   ‚Ä¢ Textos vac√≠os iniciales: {original_empty}\")\n",
    "        print(f\"   ‚Ä¢ Textos vac√≠os finales: {processed_empty}\")\n",
    "    \n",
    "    print(\"Procesamiento de caracter√≠sticas completado exitosamente\")\n",
    "\n",
    "# Ejecutar exportaci√≥n\n",
    "if 'df_with_features' in locals() and df_with_features is not None:\n",
    "    success = export_processed_dataset(df_with_features, BASE_DIR)\n",
    "    \n",
    "    if success and 'df' in locals():\n",
    "        generate_processing_report(df, df_with_features)\n",
    "else:\n",
    "    print(\"No hay dataset con caracter√≠sticas para exportar\")\n",
    "    print(\"   Aseg√∫rate de haber ejecutado las celdas anteriores correctamente\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
